{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarun-jain98/DemoApp/blob/master/Assingment5/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2KlOspXwSCc",
        "colab_type": "text"
      },
      "source": [
        "#Imports all the basic requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SrnPc0M3vji",
        "colab_type": "code",
        "outputId": "400b790c-6272-46b3-db4d-0b58fdd7b5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# example of standardizing a image dataset\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape dataset to have a single channel\n",
        "width, height, channels = X_train.shape[1], X_train.shape[2], 1\n",
        "X_train = X_train.reshape((X_train.shape[0], width, height, channels))\n",
        "X_test = X_test.reshape((X_test.shape[0], width, height, channels))\n",
        "# report pixel means and standard deviations\n",
        "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (X_train.mean(), X_train.std(), X_test.mean(), X_test.std()))\n",
        "# create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "# calculate the mean on the training dataset\n",
        "datagen.fit(X_train)\n",
        "print('Data Generator mean=%.3f, std=%.3f' % (datagen.mean, datagen.std))\n",
        "# demonstrate effect on a single batch of samples\n",
        "iterator = datagen.flow(X_train, y_train, batch_size=64)\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())\n",
        "# demonstrate effect on entire training dataset\n",
        "iterator = datagen.flow(X_train, y_train, batch_size=len(X_train), shuffle=False)\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(X_test)\n",
        "\n",
        "iterator = datagen.flow(X_test, y_test, batch_size=64)\n",
        "\n",
        "testX, testy = iterator.next()\n",
        "\n",
        "iterator = datagen.flow(X_test, y_test, batch_size=len(X_train), shuffle=False)\n",
        "\n",
        "testX, testy = iterator.next()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics train=33.318 (78.567), test=33.791 (79.172)\n",
            "Data Generator mean=33.318, std=78.567\n",
            "(64, 28, 28, 1) -0.043359917 0.9497284\n",
            "(60000, 28, 28, 1) -3.4560264e-07 0.9999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMIS-g-EwXzH",
        "colab_type": "text"
      },
      "source": [
        "#Add image normalization to training as well as test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchX = batchX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "batchX /= 255\n",
        "testX /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchy[:10]\n",
        "batchy = np_utils.to_categorical(batchy, 10)\n",
        "testy = np_utils.to_categorical(testy, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "59674dcd-9c84-40d4-f330-fae035fde47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "batchy[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "ee758220-0f60-47b7-d356-a2b3238dd0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1193
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Activation\n",
        "from keras import regularizers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1),kernel_regularizer=regularizers.l2(0.01))) #26\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))) #24\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu',kernel_regularizer=regularizers.l2(0.01))) #22\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01)))#9\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01)))#7\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01)))#5\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01)))#3\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 4, 4,kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "# keras.regularizers.l2(0.01)\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1..., kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_49 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,476\n",
            "Trainable params: 16,296\n",
            "Non-trainable params: 180\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4), kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MCxs81-wgKw",
        "colab_type": "text"
      },
      "source": [
        "#The basic model architecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "07f9bd3c-d18c-4205-c365-46e0a562abd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4131
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "filepath=\"weights-improvement.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "lr=LearningRateScheduler(scheduler, verbose=1)\n",
        "callbacks_list = [lr,checkpoint]\n",
        "\n",
        "model.fit(batchX, batchy, batch_size=128, epochs=40, verbose=1, validation_data=(testX, testy), callbacks=callbacks_list)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.8296 - acc: 0.8619 - val_loss: 0.6090 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.85800, saving model to weights-improvement.hdf5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.3254 - acc: 0.9308 - val_loss: 0.5376 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.85800 to 0.86720, saving model to weights-improvement.hdf5\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.2794 - acc: 0.9368 - val_loss: 0.2154 - val_acc: 0.9645\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.86720 to 0.96450, saving model to weights-improvement.hdf5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.2572 - acc: 0.9398 - val_loss: 0.1811 - val_acc: 0.9738\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.96450 to 0.97380, saving model to weights-improvement.hdf5\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.2371 - acc: 0.9416 - val_loss: 0.5673 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.97380\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.2245 - acc: 0.9435 - val_loss: 0.1374 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.97380 to 0.98290, saving model to weights-improvement.hdf5\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.2160 - acc: 0.9449 - val_loss: 0.5359 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98290\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.2034 - acc: 0.9470 - val_loss: 0.3742 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98290\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1970 - acc: 0.9471 - val_loss: 1.0051 - val_acc: 0.7128\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98290\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1980 - acc: 0.9455 - val_loss: 0.2584 - val_acc: 0.9377\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98290\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1853 - acc: 0.9485 - val_loss: 0.1237 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98290\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1797 - acc: 0.9496 - val_loss: 0.3063 - val_acc: 0.9200\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.98290\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1786 - acc: 0.9493 - val_loss: 0.1293 - val_acc: 0.9783\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.98290\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1699 - acc: 0.9509 - val_loss: 0.1468 - val_acc: 0.9732\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.98290\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1699 - acc: 0.9501 - val_loss: 0.0914 - val_acc: 0.9880\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.98290 to 0.98800, saving model to weights-improvement.hdf5\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1660 - acc: 0.9499 - val_loss: 0.0890 - val_acc: 0.9878\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.98800\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1635 - acc: 0.9512 - val_loss: 0.0951 - val_acc: 0.9872\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.98800\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1596 - acc: 0.9513 - val_loss: 0.0868 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.98800 to 0.98810, saving model to weights-improvement.hdf5\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1569 - acc: 0.9523 - val_loss: 0.1048 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.98810\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1561 - acc: 0.9515 - val_loss: 0.0828 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.98810 to 0.98820, saving model to weights-improvement.hdf5\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1531 - acc: 0.9529 - val_loss: 0.0895 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.98820\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1527 - acc: 0.9511 - val_loss: 0.0767 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.98820 to 0.98950, saving model to weights-improvement.hdf5\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1495 - acc: 0.9524 - val_loss: 0.0721 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.98950 to 0.99100, saving model to weights-improvement.hdf5\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1475 - acc: 0.9529 - val_loss: 0.1325 - val_acc: 0.9716\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99100\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1448 - acc: 0.9543 - val_loss: 0.0715 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99100\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1442 - acc: 0.9543 - val_loss: 0.0713 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.99100 to 0.99130, saving model to weights-improvement.hdf5\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1420 - acc: 0.9537 - val_loss: 0.0697 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99130\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1447 - acc: 0.9521 - val_loss: 0.0683 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99130\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1394 - acc: 0.9542 - val_loss: 0.0814 - val_acc: 0.9864\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99130\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1386 - acc: 0.9554 - val_loss: 0.0951 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99130\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1369 - acc: 0.9554 - val_loss: 0.0661 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99130\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1365 - acc: 0.9541 - val_loss: 0.0627 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.99130 to 0.99230, saving model to weights-improvement.hdf5\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1356 - acc: 0.9549 - val_loss: 0.0808 - val_acc: 0.9871\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99230\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1357 - acc: 0.9541 - val_loss: 0.0854 - val_acc: 0.9850\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99230\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1344 - acc: 0.9545 - val_loss: 0.0727 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99230\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1322 - acc: 0.9554 - val_loss: 0.0656 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99230\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.1346 - acc: 0.9539 - val_loss: 0.0635 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99230\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1314 - acc: 0.9548 - val_loss: 0.0882 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99230\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1306 - acc: 0.9557 - val_loss: 0.0741 - val_acc: 0.9873\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99230\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.1286 - acc: 0.9572 - val_loss: 0.0748 - val_acc: 0.9878\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4defce39e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "e6553d05-74df-4f03-b119-cde3fd76c658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('weights-improvement.hdf5')\n",
        "score = model.evaluate(testX, testy, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06274995970129967, 0.9923]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qDl21ozBnW",
        "colab_type": "code",
        "outputId": "1ef54758-7117-4263-94da-5e25e71573ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "l=[]\n",
        "misclassified = np.nonzero(model.predict_classes(X_test).reshape((-1,)) != y_test)\n",
        "for i in range(25):\n",
        "  l.append(misclassified[0][i])\n",
        "print(l) \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "for idx in range(25):  \n",
        "  v=l[idx]\n",
        "  plt.subplot(5,5,idx+1)\n",
        "  plt.imshow(X_test[v])\n",
        "  "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 5, 6, 7, 10, 13, 14, 20, 21, 25, 28, 29, 31, 36, 37, 39, 40, 42, 46, 55, 57, 61, 62, 65]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PgTXuK3l0pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}